<!DOCTYPE html><!--GNi_Lko8lZQNrsFQAyFZ_--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/logo.png" as="image"/><link rel="stylesheet" href="/_next/static/chunks/27e028834cc66852.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/268983ca9e2831a7.js"/><script src="/_next/static/chunks/aa0a7c5c022cbeb9.js" async=""></script><script src="/_next/static/chunks/3e71bad4100e84cb.js" async=""></script><script src="/_next/static/chunks/249261e921aeebba.js" async=""></script><script src="/_next/static/chunks/turbopack-7bdbf0079ae25277.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/7340adf74ff47ec0.js" async=""></script><script src="/_next/static/chunks/796e69ae18b2784c.js" async=""></script><title>XFINITE-OCR | Professional Document Intelligence</title><meta name="description" content="Advanced Neural OCR &amp; Document Analysis Dashboard"/><link rel="icon" href="/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased"><div hidden=""><!--$--><!--/$--></div><div id="app"><header class="navbar"><div class="nav-left"><div class="logo"><img src="/logo.png" alt="Xfinite" class="logo-img"/><span>Xfinite</span></div><nav class="nav-links"><a href="/">Dashboard</a><a class="active" href="/docs">Docs</a></nav></div><div class="nav-right"><button class="nav-btn-primary">Get Started</button></div></header><div class="layout-body"><aside class="sidebar-new"><div class="sidebar-section"><div class="section-header"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg><span>Documentation</span></div></div><div class="sidebar-section"><p class="sidebar-label">TABLE OF CONTENTS</p><div class="history-list"><a href="#overview" class="history-item"><span>üìä OCR Models Overview</span></a><a href="#features" class="history-item"><span>üß† What Each Model Does</span></a><a href="#inference" class="history-item"><span>‚ö° Inference &amp; Compute</span></a><a href="#comparison" class="history-item"><span>üß© Feature Summary</span></a><a href="#recommendations" class="history-item"><span>üß† Recommendations</span></a></div></div></aside><main class="dashboard-main docs-content"><div class="dashboard-header"><h1>üìö OCR Models Documentation</h1><p class="dashboard-subtitle">A comparative OCR model cheat-sheet for XFINITE-OCR (2025‚Äì2026)</p></div><article class="docs-article"><section id="overview"><h2>üìä OCR Models Overview</h2><div class="docs-table-wrapper"><table class="docs-table"><thead><tr><th>Model</th><th>Type</th><th>Core OCR</th><th>Other Features</th><th>Compute Notes</th><th>Best Fit</th></tr></thead><tbody><tr><td><strong>XF1</strong></td><td>Pipeline-based</td><td>Standard text OCR</td><td>External layout + table modules</td><td>Modular pipeline</td><td>Baseline simple OCR</td></tr><tr><td><strong>XF2</strong></td><td>Classic OCR toolkit</td><td>Line-level OCR, 90+ languages</td><td>Layout analysis, reading order, tables</td><td>CPU-friendly, GPU improves speed</td><td>Multilingual document OCR</td></tr><tr><td><strong>XF3 Vision</strong></td><td>Vision-Language Model (0.9B)</td><td>OCR + parsing (tables, formulas, charts)</td><td>Layout, reading order, Markdown/JSON</td><td>Resource-efficient, GPU/TPU</td><td>Complex layouts, multilingual</td></tr><tr><td><strong>XF3 Pro</strong></td><td>End-to-end VLM (1B)</td><td>OCR + spotting (bbox coords)</td><td>Parsing, IE, VQA, translation</td><td>~1B params, one-shot inference</td><td>End-to-end extraction</td></tr><tr><td><strong>XF3 Large</strong></td><td>Compressed context VLM (3B)</td><td>High-accuracy OCR</td><td>Long-context compression</td><td>~97% accuracy, high-res efficient</td><td>Large document collections</td></tr></tbody></table></div></section><section id="features"><h2>üß† What Each Model Does</h2><div class="model-doc-card"><h3>üìç XF1 </h3><p><strong>Type:</strong> Traditional pipeline toolkit combining modules (text detection + layout + tables).</p><p><strong>Features:</strong> Basic OCR text extraction and structured extraction via modular tools.</p><p><strong>Speed:</strong> Depends on each component; not top performer vs VLMs.</p><p><strong>Use When:</strong> You need flexible modular OCR with integration to custom components.</p></div><div class="model-doc-card"><h3>‚òÄÔ∏è XF2 </h3><p><strong>Type:</strong> Python OCR toolkit with multilingual detection (~90+ languages).</p><ul><li>Line OCR + bounding boxes</li><li>Layout detection (headers, tables, images)</li><li>Reading order detection</li><li>Table recognition modules</li></ul><p><strong>Best For:</strong> Lightweight document OCR with layout &amp; reading order when LLM integration is not needed.</p></div><div class="model-doc-card"><h3>üìò XF3 Vision</h3><p><strong>Type:</strong> Vision-Language Model (VLM) built for full document parsing.</p><ul><li>Recognizes tables, formulas, charts, reading order</li><li>Supports 109 languages (Latin, Cyrillic, Devanagari, Arabic, etc)</li><li>Outputs structured JSON/Markdown</li></ul><p><strong>Compute:</strong> ~0.9B parameters ‚Üí good trade-off for structured output without huge GPUs</p></div><div class="model-doc-card"><h3>üêâ XF3 Pro </h3><p><strong>Type:</strong> End-to-end OCR &amp; VLM, trained with RL.</p><ul><li>Text spotting (locations)</li><li>Multilingual translation of text images (14+ languages)</li><li>Information extraction (IE), VQA</li></ul><p><strong>Compute:</strong> ~1B params ‚Üí relatively lightweight for a VLM</p></div><div class="model-doc-card"><h3>üß† XF3 Large </h3><p><strong>Type:</strong> VLM optimized for optical token compression</p><ul><li>OCR text extraction with high fidelity</li><li>Efficient vision token modeling</li><li>Good for large high-resolution docs</li></ul><p><strong>Compute:</strong> Uses a specialized encoder + decoder to compress long images into fewer tokens ‚Üí faster across big documents with maintained accuracy (~97% decoded precision).</p></div></section><section id="inference"><h2>‚ö° Inference Time &amp; Compute</h2><p class="docs-note">Note: Precise ms/page numbers vary enormously depending on hardware, image size, resolution, and end-task.</p><div class="docs-table-wrapper"><table class="docs-table"><thead><tr><th>Model</th><th>Estimated Compute</th><th>Latency Trend</th><th>Notes</th></tr></thead><tbody><tr><td>XF2 (Surya)</td><td>Low-Med</td><td>‚≠ê‚≠ê</td><td>Runs on CPU or GPU; basic components</td></tr><tr><td>XF1 (Marker)</td><td>Med</td><td>‚≠ê‚≠ê</td><td>Depends on each pipeline part</td></tr><tr><td>XF3 Vision (0.9B)</td><td>Med</td><td>‚≠ê‚≠ê‚≠ê</td><td>Fast for VLM parsing; resource-efficient</td></tr><tr><td>XF3 Pro (~1B)</td><td>Med</td><td>‚≠ê‚≠ê‚≠ê</td><td>End-to-end one pass OCR + tasks</td></tr><tr><td>XF3 Large (~3B)</td><td>Med-High</td><td>‚≠ê‚≠ê‚≠ê</td><td>Compresses tokens; good for high-res</td></tr></tbody></table></div><p class="docs-note">Typical rule: under 1B parameters ‚âà great for page-by-page inference on mid-range GPUs/CPUs; 3B+ models need stronger GPUs but give deeper context/accuracy. üìà</p></section><section id="comparison"><h2>üß© Feature Summary</h2><div class="docs-table-wrapper"><table class="docs-table"><thead><tr><th>Model</th><th>OCR Text</th><th>Spotting (bbox)</th><th>Layout Parsing</th><th>Translation</th><th>IE/VQA</th></tr></thead><tbody><tr><td>XF2 (Surya)</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚ùå</td><td>‚ùå</td></tr><tr><td>XF1 (Marker)</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚ùå</td><td>‚ùå</td></tr><tr><td>XF3 Vision</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>üü°</td><td>üü°</td></tr><tr><td>XF3 Pro</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td></tr><tr><td>XF3 Large</td><td>‚úîÔ∏è</td><td>‚úîÔ∏è</td><td>‚ùì</td><td>‚ùì</td><td>‚ùì</td></tr></tbody></table></div><p class="docs-note">üü° = inferred capability thanks to structured outputs and VLM context, actual performance depends on integration/finetuning.</p></section><section id="recommendations"><h2>üß† Practical Recommendations</h2><ul class="docs-recs"><li><strong>‚ú® Best for basic OCR pipelines:</strong> XF2 ‚Äî lightweight, works well with CPU &amp; GPU, easy to integrate.</li><li><strong>üìÑ Best for structured document parsing:</strong> XF3 Vision ‚Äî excellent at tables, formulas, reading order, multilingual documents.</li><li><strong>üß† Best for semantic OCR + translation + IE:</strong> XF3 Pro ‚Äî unified end-to-end model with spotting and translation.</li><li><strong>üìà Best for large archive throughput:</strong> XF3 Large ‚Äî efficient token compression + high accuracy when processing many pages.</li><li><strong>üõ†Ô∏è Best for customizable modular workflows:</strong> XF1 ‚Äî use when you want explicit control over each stage.</li></ul></section></article></main></div></div><!--$--><!--/$--><script src="/_next/static/chunks/268983ca9e2831a7.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7340adf74ff47ec0.js\"],\"default\"]\n3:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7340adf74ff47ec0.js\"],\"default\"]\n4:I[22016,[\"/_next/static/chunks/796e69ae18b2784c.js\"],\"\"]\nf:I[68027,[],\"default\"]\n:HL[\"/_next/static/chunks/27e028834cc66852.css\",\"style\"]\n:HL[\"/logo.png\",\"image\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"GNi-Lko8lZQNrsFQAyFZ-\",\"c\":[\"\",\"docs\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"docs\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/27e028834cc66852.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"id\":\"app\",\"children\":[[\"$\",\"header\",null,{\"className\":\"navbar\",\"children\":[[\"$\",\"div\",null,{\"className\":\"nav-left\",\"children\":[[\"$\",\"div\",null,{\"className\":\"logo\",\"children\":[[\"$\",\"img\",null,{\"src\":\"/logo.png\",\"alt\":\"Xfinite\",\"className\":\"logo-img\"}],[\"$\",\"span\",null,{\"children\":\"Xfinite\"}]]}],[\"$\",\"nav\",null,{\"className\":\"nav-links\",\"children\":[[\"$\",\"$L4\",null,{\"href\":\"/\",\"children\":\"Dashboard\"}],[\"$\",\"$L4\",null,{\"href\":\"/docs\",\"className\":\"active\",\"children\":\"Docs\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"nav-right\",\"children\":[\"$\",\"button\",null,{\"className\":\"nav-btn-primary\",\"children\":\"Get Started\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"layout-body\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"sidebar-new\",\"children\":[[\"$\",\"div\",null,{\"className\":\"sidebar-section\",\"children\":[\"$\",\"div\",null,{\"className\":\"section-header\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"18\",\"height\":\"18\",\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":\"2\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M4 19.5A2.5 2.5 0 0 1 6.5 17H20\"}],[\"$\",\"path\",null,{\"d\":\"M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z\"}]]}],[\"$\",\"span\",null,{\"children\":\"Documentation\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"sidebar-section\",\"children\":[[\"$\",\"p\",null,{\"className\":\"sidebar-label\",\"children\":\"TABLE OF CONTENTS\"}],[\"$\",\"div\",null,{\"className\":\"history-list\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#overview\",\"className\":\"history-item\",\"children\":[\"$\",\"span\",null,{\"children\":\"üìä OCR Models Overview\"}]}],[\"$\",\"a\",null,{\"href\":\"#features\",\"className\":\"history-item\",\"children\":[\"$\",\"span\",null,{\"children\":\"üß† What Each Model Does\"}]}],[\"$\",\"a\",null,{\"href\":\"#inference\",\"className\":\"history-item\",\"children\":[\"$\",\"span\",null,{\"children\":\"‚ö° Inference \u0026 Compute\"}]}],[\"$\",\"a\",null,{\"href\":\"#comparison\",\"className\":\"history-item\",\"children\":[\"$\",\"span\",null,{\"children\":\"üß© Feature Summary\"}]}],[\"$\",\"a\",null,{\"href\":\"#recommendations\",\"className\":\"history-item\",\"children\":[\"$\",\"span\",null,{\"children\":\"üß† Recommendations\"}]}]]}]]}]]}],[\"$\",\"main\",null,{\"className\":\"dashboard-main docs-content\",\"children\":[[\"$\",\"div\",null,{\"className\":\"dashboard-header\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"üìö OCR Models Documentation\"}],[\"$\",\"p\",null,{\"className\":\"dashboard-subtitle\",\"children\":\"A comparative OCR model cheat-sheet for XFINITE-OCR (2025‚Äì2026)\"}]]}],[\"$\",\"article\",null,{\"className\":\"docs-article\",\"children\":[[\"$\",\"section\",null,{\"id\":\"overview\",\"children\":[[\"$\",\"h2\",null,{\"children\":\"üìä OCR Models Overview\"}],[\"$\",\"div\",null,{\"className\":\"docs-table-wrapper\",\"children\":[\"$\",\"table\",null,{\"className\":\"docs-table\",\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Type\"}],[\"$\",\"th\",null,{\"children\":\"Core OCR\"}],[\"$\",\"th\",null,{\"children\":\"Other Features\"}],\"$L5\",\"$L6\"]}]}],\"$L7\"]}]}]]}],\"$L8\",\"$L9\",\"$La\",\"$Lb\"]}]]}]]}]]}],[\"$Lc\"],\"$Ld\"]}],{},null,false,false]},null,false,false]},null,false,false],\"$Le\",false]],\"m\":\"$undefined\",\"G\":[\"$f\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7340adf74ff47ec0.js\"],\"OutletBoundary\"]\n11:\"$Sreact.suspense\"\n13:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7340adf74ff47ec0.js\"],\"ViewportBoundary\"]\n15:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7340adf74ff47ec0.js\"],\"MetadataBoundary\"]\n5:[\"$\",\"th\",null,{\"children\":\"Compute Notes\"}]\n6:[\"$\",\"th\",null,{\"children\":\"Best Fit\"}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"XF1\"}]}],[\"$\",\"td\",null,{\"children\":\"Pipeline-based\"}],[\"$\",\"td\",null,{\"children\":\"Standard text OCR\"}],[\"$\",\"td\",null,{\"children\":\"External layout + table modules\"}],[\"$\",\"td\",null,{\"children\":\"Modular pipeline\"}],[\"$\",\"td\",null,{\"children\":\"Baseline simple OCR\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"XF2\"}]}],[\"$\",\"td\",null,{\"children\":\"Classic OCR toolkit\"}],[\"$\",\"td\",null,{\"children\":\"Line-level OCR, 90+ languages\"}],[\"$\",\"td\",null,{\"children\":\"Layout analysis, reading order, tables\"}],[\"$\",\"td\",null,{\"children\":\"CPU-friendly, GPU improves speed\"}],[\"$\",\"td\",null,{\"children\":\"Multilingual document OCR\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"XF3 Vision\"}]}],[\"$\",\"td\",null,{\"children\":\"Vision-Language Model (0.9B)\"}],[\"$\",\"td\",null,{\"children\":\"OCR + parsing (tables, formulas, charts)\"}],[\"$\",\"td\",null,{\"children\":\"Layout, reading order, Markdown/JSON\"}],[\"$\",\"td\",null,{\"children\":\"Resource-efficient, GPU/TPU\"}],[\"$\",\"td\",null,{\"children\":\"Complex layouts, multilingual\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"XF3 Pro\"}]}],[\"$\",\"td\",null,{\"children\":\"End-to-end VLM (1B)\"}],[\"$\",\"td\",null,{\"children\":\"OCR + spotting (bbox coords)\"}],[\"$\",\"td\",null,{\"children\":\"Parsing, IE, VQA, translation\"}],[\"$\",\"td\",null,{\"children\":\"~1B params, one-shot inference\"}],[\"$\",\"td\",null,{\"children\":\"End-to-end extraction\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"XF3 Large\"}]}],[\"$\",\"td\",null,{\"children\":\"Compressed context VLM (3B)\"}],[\"$\",\"td\",null,{\"children\":\"High-accuracy OCR\"}],[\"$\",\"td\",null,{\"children\":\"Long-context compression\"}],[\"$\",\"td\",null,{\"children\":\"~97% accuracy, high-res efficient\"}],[\"$\",\"td\",null,{\"children\":\"Large document collections\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"section\",null,{\"id\":\"features\",\"children\":[[\"$\",\"h2\",null,{\"children\":\"üß† What Each Model Does\"}],[\"$\",\"div\",null,{\"className\":\"model-doc-card\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"üìç XF1 \"}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Type:\"}],\" Traditional pipeline toolkit combining modules (text detection + layout + tables).\"]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Features:\"}],\" Basic OCR text extraction and structured extraction via modular tools.\"]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Speed:\"}],\" Depends on each component; not top performer vs VLMs.\"]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Use When:\"}],\" You need flexible modular OCR with integration to custom components.\"]}]]}],[\"$\",\"div\",null,{\"className\":\"model-doc-card\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"‚òÄÔ∏è XF2 \"}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Type:\"}],\" Python OCR toolkit with multilingual detection (~90+ languages).\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Line OCR + bounding boxes\"}],[\"$\",\"li\",null,{\"children\":\"Layout detection (headers, tables, images)\"}],[\"$\",\"li\",null,{\"children\":\"Reading order detection\"}],[\"$\",\"li\",null,{\"children\":\"Table recognition modules\"}]]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Best For:\"}],\" Lightweight document OCR with layout \u0026 reading order when LLM integration is not needed.\"]}]]}],[\"$\",\"div\",null,{\"className\":\"model-doc-card\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"üìò XF3 Vision\"}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Type:\"}],\" Vision-Language Model (VLM) built for full document parsing.\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Recognizes tables, formulas, charts, reading order\"}],[\"$\",\"li\",null,{\"children\":\"Supports 109 languages (Latin, Cyrillic, Devanagari, Arabic, etc)\"}],[\"$\",\"li\",null,{\"children\":\"Outputs structured JSON/Markdown\"}]]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Compute:\"}],\" ~0.9B parameters ‚Üí good trade-off for structured output without huge GPUs\"]}]]}],[\"$\",\"div\",null,{\"className\":\"model-doc-card\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"üêâ XF3 Pro \"}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Type:\"}],\" End-to-end OCR \u0026 VLM, trained with RL.\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Text spotting (locations)\"}],[\"$\",\"li\",null,{\"children\":\"Multilingual translation of text images (14+ languages)\"}],[\"$\",\"li\",null,{\"children\":\"Information extraction (IE), VQA\"}]]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Compute:\"}],\" ~1B params ‚Üí relatively lightweight for a VLM\"]}]]}],[\"$\",\"div\",null,{\"className\":\"model-doc-card\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"üß† XF3 Large \"}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Type:\"}],\" VLM optimized for optical token compression\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"OCR text extraction with high fidelity\"}],[\"$\",\"li\",null,{\"children\":\"Efficient vision token modeling\"}],[\"$\",\"li\",null,{\"children\":\"Good for large high-resolution docs\"}]]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Compute:\"}],\" Uses a specialized encoder + decoder to compress long images into fewer tokens ‚Üí faster across big documents with maintained accuracy (~97% decoded precision).\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"section\",null,{\"id\":\"inference\",\"children\":[[\"$\",\"h2\",null,{\"children\":\"‚ö° Inference Time \u0026 Compute\"}],[\"$\",\"p\",null,{\"className\":\"docs-note\",\"children\":\"Note: Precise ms/page numbers vary enormously depending on hardware, image size, resolution, and end-task.\"}],[\"$\",\"div\",null,{\"className\":\"docs-table-wrapper\",\"children\":[\"$\",\"table\",null,{\"className\":\"docs-table\",\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Estimated Compute\"}],[\"$\",\"th\",null,{\"children\":\"Latency Trend\"}],[\"$\",\"th\",null,{\"children\":\"Notes\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF2 (Surya)\"}],[\"$\",\"td\",null,{\"children\":\"Low-Med\"}],[\"$\",\"td\",null,{\"children\":\"‚≠ê‚≠ê\"}],[\"$\",\"td\",null,{\"children\":\"Runs on CPU or GPU; basic components\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF1 (Marker)\"}],[\"$\",\"td\",null,{\"children\":\"Med\"}],[\"$\",\"td\",null,{\"children\":\"‚≠ê‚≠ê\"}],[\"$\",\"td\",null,{\"children\":\"Depends on each pipeline part\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF3 Vision (0.9B)\"}],[\"$\",\"td\",null,{\"children\":\"Med\"}],[\"$\",\"td\",null,{\"children\":\"‚≠ê‚≠ê‚≠ê\"}],[\"$\",\"td\",null,{\"children\":\"Fast for VLM parsing; resource-efficient\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF3 Pro (~1B)\"}],[\"$\",\"td\",null,{\"children\":\"Med\"}],[\"$\",\"td\",null,{\"children\":\"‚≠ê‚≠ê‚≠ê\"}],[\"$\",\"td\",null,{\"children\":\"End-to-end one pass OCR + tasks\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF3 Large (~3B)\"}],[\"$\",\"td\",null,{\"children\":\"Med-High\"}],[\"$\",\"td\",null,{\"children\":\"‚≠ê‚≠ê‚≠ê\"}],[\"$\",\"td\",null,{\"children\":\"Compresses tokens; good for high-res\"}]]}]]}]]}]}],[\"$\",\"p\",null,{\"className\":\"docs-note\",\"children\":\"Typical rule: under 1B parameters ‚âà great for page-by-page inference on mid-range GPUs/CPUs; 3B+ models need stronger GPUs but give deeper context/accuracy. üìà\"}]]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"section\",null,{\"id\":\"comparison\",\"children\":[[\"$\",\"h2\",null,{\"children\":\"üß© Feature Summary\"}],[\"$\",\"div\",null,{\"className\":\"docs-table-wrapper\",\"children\":[\"$\",\"table\",null,{\"className\":\"docs-table\",\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"OCR Text\"}],[\"$\",\"th\",null,{\"children\":\"Spotting (bbox)\"}],[\"$\",\"th\",null,{\"children\":\"Layout Parsing\"}],[\"$\",\"th\",null,{\"children\":\"Translation\"}],[\"$\",\"th\",null,{\"children\":\"IE/VQA\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF2 (Surya)\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚ùå\"}],[\"$\",\"td\",null,{\"children\":\"‚ùå\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF1 (Marker)\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚ùå\"}],[\"$\",\"td\",null,{\"children\":\"‚ùå\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF3 Vision\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"üü°\"}],[\"$\",\"td\",null,{\"children\":\"üü°\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF3 Pro\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XF3 Large\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚úîÔ∏è\"}],[\"$\",\"td\",null,{\"children\":\"‚ùì\"}],[\"$\",\"td\",null,{\"children\":\"‚ùì\"}],[\"$\",\"td\",null,{\"children\":\"‚ùì\"}]]}]]}]]}]}],[\"$\",\"p\",null,{\"className\":\"docs-note\",\"children\":\"üü° = inferred capability thanks to structured outputs and VLM context, actual performance depends on integration/finetuning.\"}]]}]\n"])</script><script>self.__next_f.push([1,"b:[\"$\",\"section\",null,{\"id\":\"recommendations\",\"children\":[[\"$\",\"h2\",null,{\"children\":\"üß† Practical Recommendations\"}],[\"$\",\"ul\",null,{\"className\":\"docs-recs\",\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"‚ú® Best for basic OCR pipelines:\"}],\" XF2 ‚Äî lightweight, works well with CPU \u0026 GPU, easy to integrate.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"üìÑ Best for structured document parsing:\"}],\" XF3 Vision ‚Äî excellent at tables, formulas, reading order, multilingual documents.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"üß† Best for semantic OCR + translation + IE:\"}],\" XF3 Pro ‚Äî unified end-to-end model with spotting and translation.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"üìà Best for large archive throughput:\"}],\" XF3 Large ‚Äî efficient token compression + high accuracy when processing many pages.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"üõ†Ô∏è Best for customizable modular workflows:\"}],\" XF1 ‚Äî use when you want explicit control over each stage.\"]}]]}]]}]\nc:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/796e69ae18b2784c.js\",\"async\":true,\"nonce\":\"$undefined\"}]\nd:[\"$\",\"$L10\",null,{\"children\":[\"$\",\"$11\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@12\"}]}]\ne:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L13\",null,{\"children\":\"$L14\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L15\",null,{\"children\":[\"$\",\"$11\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L16\"}]}]}],null]}]\n"])</script><script>self.__next_f.push([1,"14:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"17:I[27201,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7340adf74ff47ec0.js\"],\"IconMark\"]\n12:null\n16:[[\"$\",\"title\",\"0\",{\"children\":\"XFINITE-OCR | Professional Document Intelligence\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Advanced Neural OCR \u0026 Document Analysis Dashboard\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L17\",\"3\",{}]]\n"])</script></body></html>